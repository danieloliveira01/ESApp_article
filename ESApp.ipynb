{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM,Bidirectional,Dropout\n",
    "from keras.layers import AveragePooling1D\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.layers import Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import * \n",
    "from sklearn.model_selection import KFold \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reqTxt.csv', header=None) #Report or file containing the set of training and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRequire = df.iloc[:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfRequire.shape)\n",
    "print(dfRequire.columns)\n",
    "X = dfRequire[0]\n",
    "print(X[0])\n",
    "X = np.array(X)\n",
    "\n",
    "print(len(X))\n",
    "\n",
    "print('Train and test dataset loaded...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('estiDeep.data', header=None) #File containing the set of training and test labels.\n",
    "y = np.array(y)\n",
    "print ('Shape of label tensor:', y.shape)\n",
    "print(y.dtype)\n",
    "\n",
    "#Number of texts in train and test dataset \n",
    "MAX_LEN = 23313\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1000) \n",
    "kf.get_n_splits(X) #returns the number of splitting iterations in the cross-validator\n",
    "print(kf) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando Embeddings ja convertidos\n",
    "O arquivo Convert embeddings mostra a conversão da base de requisitos em embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pret_model = pd.read_csv('embeddings_finetuned_FastText.csv', delimiter= ',', header=None) #insert embedding \n",
    "embedding_matrix = pret_model.iloc[0:23313,:] \n",
    "dfEmbedding_mat = pd.DataFrame(embedding_matrix)\n",
    "embedding_mat = dfEmbedding_mat.fillna('0') \n",
    "\n",
    "print('Embedding mat: ' + str(embedding_mat.shape))\n",
    "\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_test_Deep = []\n",
    "all_y_pred_Deep = []\n",
    "vetMAEDeep = []\n",
    "vetR2Deep = []\n",
    "vetMSEDeep = []\n",
    "vetMdaeDeep = []\n",
    "vetPred25Deep = []\n",
    "\n",
    "#Pred(25)\n",
    "def calcular_pred_25(y_true, y_pred):\n",
    "    abs_errors = np.abs(y_true - y_pred)\n",
    "    pred_25 = np.mean((abs_errors / np.abs(y_true)) <= 0.25) * 100\n",
    "    return pred_25\n",
    "\n",
    "#Cross-validation loop\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x_train, test_x = X[train_index], X[test_index]\n",
    "    train_y, test_y = y[train_index], y[test_index]\n",
    "    \n",
    "    texts_train = x_train.astype(str)\n",
    "    texts_test = test_x.astype(str)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_LEN, char_level=False, lower=False) \n",
    "    tokenizer.fit_on_texts(texts_train)                            \n",
    "    encSequences = tokenizer.texts_to_sequences(texts_train)          \n",
    "    encSequences_test = tokenizer.texts_to_sequences(texts_test)      \n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1 \n",
    "    print('Vocab_size: ' + str(vocab_size))\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100  \n",
    "\n",
    "    x_train = pad_sequences(encSequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    x_test = pad_sequences(encSequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    print('Shape of data tensor:', x_train.shape)\n",
    "    print('Shape of data test tensor:', x_test.shape)\n",
    "\n",
    "    # Definindo o modelo\n",
    "    model = Sequential()\n",
    "\n",
    "    #embedding = Embedding(MAX_LEN, 300, input_length=MAX_SEQUENCE_LENGTH, trainable=True) #Using this for FastText\n",
    "    embedding = Embedding(MAX_LEN, 768, input_length=MAX_SEQUENCE_LENGTH, trainable=True) #Using this for XLNET\n",
    "    \n",
    "    embedding.build(input_shape=(None,))  #input_shape é ajustado para (None,) para batch size variável\n",
    "    embedding.set_weights([embedding_mat])\n",
    "    model.add(embedding)\n",
    "    \n",
    "    model.add(AveragePooling1D(pool_size=100))\n",
    "    model.add(LSTM(50, dropout=0.3, recurrent_dropout=0.2, return_sequences=False)) \n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu')) \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    adam = Adam(learning_rate=0.001) \n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "    model_history = model.fit(x_train, train_y,\n",
    "              batch_size=128,\n",
    "              epochs=30, callbacks=[es],\n",
    "              validation_data=(x_test, test_y))                \n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Armazenar os resultados\n",
    "    all_y_test_Deep.extend(test_y.flatten())\n",
    "    all_y_pred_Deep.extend(y_pred.flatten())\n",
    "\n",
    "    # Cálculo das métricas\n",
    "    pred_25 = calcular_pred_25(test_y.flatten(), y_pred.flatten())\n",
    "    vetPred25Deep.append(pred_25)\n",
    "    \n",
    "    mae = mean_absolute_error(test_y, y_pred)\n",
    "    vetMAEDeep.append(mae)\n",
    "    medAE = median_absolute_error(test_y, y_pred)\n",
    "    vetMdaeDeep.append(medAE)\n",
    "    r2 = r2_score(test_y, y_pred)\n",
    "    vetR2Deep.append(r2)\n",
    "    mse = mean_squared_error(test_y, y_pred)\n",
    "    vetMSEDeep.append(mse)\n",
    "\n",
    "maeMedio = np.mean(vetMAEDeep)  \n",
    "madAEMedio = np.mean(vetMdaeDeep)  \n",
    "r2Medio = np.mean(vetR2Deep)  \n",
    "mseMedio = np.mean(vetMSEDeep) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatar os resultados\n",
    "maeMedio = np.mean(vetMAEDeep)  \n",
    "madAEMedio = np.mean(vetMdaeDeep)  \n",
    "r2Medio = np.mean(vetR2Deep)  \n",
    "mseMedio = np.mean(vetMSEDeep) \n",
    "stdMae = np.std(vetMAEDeep)\n",
    "stdr2 = np.std(vetR2Deep)\n",
    "stdMse = np.std(vetMSEDeep)\n",
    "\n",
    "\n",
    "mae_result = f\"{maeMedio:.2f} ± {stdMae:.2f}\"\n",
    "mse_result = f\"{mseMedio:.2f} ± {stdMse:.2f}\"\n",
    "r2_result = f\"{r2Medio:.2f} ± {stdr2:.2f}\"\n",
    "mad_result = f\"{madAEMedio:.2f}\" \n",
    "\n",
    "\n",
    "#Imprimir os resultados\n",
    "print('MAE:', mae_result)\n",
    "print('MSE:', mse_result)\n",
    "print('R2:', r2_result)\n",
    "print('MdAE:', mad_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
