{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings with XLnet and FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time #monitorar tempo\n",
    "import psutil#monitorar cpu\n",
    "from GPUtil import GPUtil #monitorar GPU\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "import numpy as np\n",
    "import csv\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting in Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando o dispositivo:\", device)\n",
    "\n",
    "# Carregar o modelo XLNet e o tokenizer\n",
    "xlnet_model = XLNetModel.from_pretrained('xlnet-base-cased', from_tf=False).to(device)\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "\n",
    "# Carregar modelo e tokenizer com fine-tuning\n",
    "#xlnet_model = XLNetModel.from_pretrained('xlnet_finetuned_user_stories').to(device)\n",
    "#xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet_finetuned_user_stories')\n",
    "\n",
    "def split_text(text, tokenizer, max_length=512):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    # Se o número de tokens exceder o limite, dividir o texto\n",
    "    if len(tokens) > max_length:\n",
    "        # Dividir em partes menores\n",
    "        return [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [tokens]\n",
    "\n",
    "def get_text_embedding(text, model, tokenizer, max_length=512):\n",
    "    segments = split_text(text, tokenizer, max_length)\n",
    "    embeddings = []\n",
    "    for segment in segments:\n",
    "        inputs = tokenizer.encode_plus(segment, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            embedding = hidden_states[-1][0, 0, :].detach().cpu().numpy()  # Representação do primeiro token\n",
    "            embeddings.append(embedding)\n",
    "    # Você pode escolher como combinar as embeddings (por exemplo, média)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "\n",
    "# Parâmetros de entrada e saída\n",
    "input_file = 'input.txt'\n",
    "output_file = 'output_xlnet.csv'\n",
    "\n",
    "# Processar todos os dados\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as f_out, open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "    writer = csv.writer(f_out)\n",
    "    for line in f_in:\n",
    "        text = line.strip()\n",
    "        text = ''.join(e for e in text if e.isalnum() or e.isspace())\n",
    "        try:\n",
    "            embedding = get_text_embedding(text, xlnet_model, xlnet_tokenizer)\n",
    "            writer.writerow(embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar a linha: {text}. Erro: {e}\")\n",
    "\n",
    "print(\"Conversão completa! Os embeddings foram salvos em\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerar embeddings apenas para as linhas necessárias\n",
    "\n",
    "\n",
    "#Carregar o modelo FastText\n",
    "#model_path = hf_hub_download(repo_id=\"facebook/fasttext-pt-vectors\", filename=\"model.bin\")\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\n",
    "ft_model = fasttext.load_model(model_path)\n",
    "\n",
    "#Função para obter embeddings de um texto\n",
    "def get_text_embedding(text, model, embedding_dim):\n",
    "    words = text.split()\n",
    "    embedding = np.zeros((embedding_dim,))\n",
    "    for word in words:\n",
    "        embedding += model.get_word_vector(word)\n",
    "    return embedding\n",
    "def gerarEmbeddings(input_file):\n",
    "    # Parâmetros\n",
    "    output_file = 'output_fasttext.csv'  #Arquivo de saída para os embeddings\n",
    "    embedding_dim = ft_model.get_dimension()\n",
    "    chunk_size = 1000 \n",
    "    num_labels = len(labels)\n",
    "\n",
    "    #Processar os dados em chunks\n",
    "    processed_rows = 0\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f_out, open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "        writer = csv.writer(f_out)\n",
    "        for line in f_in:\n",
    "            if processed_rows >= num_labels:\n",
    "                break\n",
    "            text = line.strip()\n",
    "            embedding = get_text_embedding(text, ft_model, embedding_dim)\n",
    "            writer.writerow(embedding)\n",
    "            processed_rows += 1\n",
    "\n",
    "    print(\"Conversão completa! Os embeddings foram salvos em\", output_file)\n",
    "\n",
    "gerarEmbeddings(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
